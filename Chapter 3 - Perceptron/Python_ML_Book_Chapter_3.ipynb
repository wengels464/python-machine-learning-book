{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python ML Book - Chapter 3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 3 - Perceptron#\n",
        "\n",
        "##Definition of artifical neurons##\n",
        "\n",
        "Artificial neurons in the context of binary classification make use of a decision function, which we will refer to as $ùõ∑(z)$ \n",
        "\n",
        "This function takes a linear combination of two vectors.\n",
        "\n",
        "*Note, when we refer to a linear combination of vectors we are speaking specifically about the vector dot product*\n",
        "\n",
        "There's the weight vector:\n",
        "\n",
        "$w_m = [w_1 ... w_m]$\n",
        "\n",
        "and the input (neuron) values:\n",
        "\n",
        "$x_m = [x_1 ... x_m] $\n",
        "\n",
        "If the net input of an example like\n",
        "\n",
        "$x^{(i)}$\n",
        "\n",
        "is greater than a certain **threshold value** which we refer to as ùú≠ (theta), then we predict class $1$. \n",
        "\n",
        "Otherwise we return class $-1$.\n",
        "\n",
        "This means that the decision function $ùõ∑(z)$ is a variant of a unit step function.\n",
        "\n",
        "In the case that:\n",
        "\n",
        "$ùõ∑(z) \\ge ùú≠$\n",
        "\n",
        "we return class $+1$\n",
        "\n",
        "Else if:\n",
        "\n",
        "$ùõ∑(z) \\lt ùú≠$\n",
        "\n",
        "we return class $-1$.\n",
        "\n",
        "In order to calculate $z$ we must initialize a weight zero and an input zero. By convention:\n",
        "\n",
        "$w_0=-ùú≠$\n",
        "\n",
        "and\n",
        "\n",
        "$x_0 = 1$\n",
        "\n",
        "\n",
        "$z$ can now be expressed as:\n",
        "\n",
        "$z=w_0x_0+w_1x_1...w_mx_m = w^Tx$\n",
        "\n",
        "$z$ therefore is the **dot product** of **w** and **x**. which we can express as:\n",
        "\n",
        "$z=w^Tx$\n",
        "\n",
        "By inserting ùú≠ into the initialized **bias unit** we are able to define $z$ in the manner shown above.\n",
        "\n",
        "The following diagram illustrates how the decision function works:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1h3rTJzYASCxLYtf8kX0KIEz8HbIdcOUB)\n",
        "\n",
        "It is clear from the example that in order for the perceptron to work the two subsets of data must be **linearly separable**. If not, the decision function will never converge on a solution.\n",
        "\n",
        "***\n",
        "\n",
        "##The Perceptron Learning Rule##\n",
        "\n",
        "These neurons are crude appeoximations of \"all or nothing\" model neurons in the human CNS. They either fire (+1) or they don't (-1).\n",
        "\n",
        "The learning algorithm can be expressed thusly:\n",
        "\n",
        "1) Initialize the weights to zero or small random numbers.\n",
        "\n",
        "2) For each training example $x^{(i)}$\n",
        "\n",
        "  *a. Compute the output value $\\hat{y}$.*\n",
        "\n",
        "  *b. Update the weights.*\n",
        "\n",
        "$w_j := w_j + Œîw_j$\n",
        "\n",
        "Thist update value of $Œîw_j$ is defined by the following formula:\n",
        "\n",
        "$Œîw_j = Œ±(y^{(i)} - \\hat{y}^{(i)})x_j^{(i)}$\n",
        "\n",
        "where $Œ±$ is our learning rate (some hyperparameter float between 0.0 and 1.0) \n",
        "\n",
        "and $y^{(i)}$ is the **true class label**, or actual observed data.\n",
        "\n",
        "${\\hat{y}}^{(i)}$ in this case is the **predicted class label**.\n",
        "\n",
        "In this case the inner formula:\n",
        "\n",
        "$(y^{(i)}-{\\hat{y}}^{(i)})$\n",
        "\n",
        "represents the **difference** between the predicted data and the evaluation data.\n",
        "\n",
        "In the case of a two dimensional dataset we would write the update function as this series:\n",
        "\n",
        "***\n",
        "\n",
        "$Œîw_0 = Œ±(y^{(i)}-output^{(i)})$\n",
        "\n",
        "$Œîw_1 = Œ±(y^{(i)}-output^{(i)})x_1^{(i)}$\n",
        "\n",
        "$Œîw_2 = Œ±(y^{(i)}-output^{(i)})x_2^{(i)}$\n",
        "\n",
        "$Œîw_{m-1} = Œ±(y^{(i)}-output^{(i)})x_{m-1}^{(i)}$\n",
        "\n",
        "***\n",
        "\n",
        "*In this case $(m-1)$ is used because $w$ and $x$ are zero-indexed...*\n",
        "\n",
        "Mathematically, it is proven that the perceptron will always converge given two conditions: linearly separable variables and sufficiently small $Œ±$.\n",
        "\n",
        "Without linearly separable variables, we must set the maximum number of total iterations **epochs** and a threshold value for total missclassifications. \n",
        "\n",
        "Without these conditions the perceptron will never converge.\n",
        "\n",
        "***\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1tIlgaqk-Qh88r2DwwASAbmNxmqcLAnQZ)\n",
        "\n",
        "***\n",
        "\n",
        "The function of the perceptron flow can be summarized as follows:\n",
        "\n",
        "***\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1cVN1fLFJ_JqXkGBUkl_bRCms-Q93SKH4)\n",
        "\n",
        "***\n",
        "\n",
        "The net input function is the solution to $z$, in this case the vector resultant from $w^Tx$.\n",
        "\n",
        "The threshold function is the familiar $\\phi z(Œ∏)$, the step function."
      ],
      "metadata": {
        "id": "s_43DQpK8EYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from numpy.ma.core import shape\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "nb3XlGBIjx7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "print(type(iris))\n",
        "# In this dataset, column 3 represents petal length and column 4 represents petal width\n",
        "# In the iris.target array, 0=Setosa, 1=Versicolor, 2=Virginica\n",
        "\n",
        "X = iris.data[:, [2,3]]\n",
        "y = iris.target\n",
        "\n",
        "print(shape(X))\n",
        "\n",
        "\"\"\"\n",
        "print(\"The shape of y is: \", shape(y))\n",
        "print(\"The shape of X is: \", shape(X))\n",
        "\"\"\"\n",
        "\n",
        "# We now break the data into 70% training and 30% eval.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
        "\n",
        "# Standardize the data features (from sklearn)\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# Verify the data is a compatible shape\n",
        "\n",
        "\"\"\"\n",
        "print(shape(X_train_std))\n",
        "print(shape(X_test_std))\n",
        "print(shape(y_train))\n",
        "\"\"\"\n",
        "\n",
        "# Train the Perceptron\n",
        "ppn = Perceptron(max_iter=40, eta0=0.1, random_state=1)\n",
        "ppn.fit(X_train_std, y_train)\n",
        "\n",
        "# Test the Perceptron\n",
        "y_pred = ppn.predict(X_train_std)\n",
        "print(shape(y_pred))\n",
        "print(shape(y_test))\n",
        "#accuracy_score(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "lh0DJnU7j-nA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ca4584-0b8c-46ce-925a-3adabbc679de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.utils.Bunch'>\n",
            "(150, 2)\n",
            "(105,)\n",
            "(45,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Create our X and y data\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "\n",
        "# Apply the scaler to the X training data\n",
        "X_train_std = sc.transform(X_train)\n",
        "\n",
        "# Apply the SAME scaler to the X test data\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# Create a perceptron object with the parameters: 40 iterations (epochs) over the data, and a learning rate of 0.1\n",
        "ppn = Perceptron(max_iter=300000, eta0=0.000001, random_state=0)\n",
        "\n",
        "# Train the perceptron\n",
        "ppn.fit(X_train_std, y_train)\n",
        "\n",
        "# Apply the trained perceptron on the X data to make predicts for the y test data\n",
        "y_pred = ppn.predict(X_test_std)\n",
        "\n",
        "# View the accuracy of the model, which is: 1 - (observations predicted wrong / total observations)\n",
        "print(shape(y_pred))\n",
        "print(shape(y_test))\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjMwFPNUx0a3",
        "outputId": "5da90d34-4d33-4910-e9ab-f61849995bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(45,)\n",
            "(45,)\n",
            "Accuracy: 0.84\n"
          ]
        }
      ]
    }
  ]
}